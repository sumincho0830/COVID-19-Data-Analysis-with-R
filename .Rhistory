install.packages('ggplot2')
q()
install.packages('ggplot2')
install.packages('ggplot2')
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('ggplot2') # visualization
library('ggthemes') # visualization
install.packages('ggthemes')
install.packages
install.packages('scale')
install.packages('dplyr')
install.packages('randomForest')
q()
library('randomForest') # classification algorithm
library('dplyr') # data manipulation
library('scales') # visualization
library('ggthemes') # visualization
library('ggplot2') # visualization
train <- read.csv('C:\codingProjects\MachineLearning', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\MachineLearning', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\ MachineLearning', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\ MachineLearning\ hai-data.xlsx', stringsAsFactors = F)
q()
train <- read.csv('C:\ codingProjects\ MachineLearning\ hai-data', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\ MachineLearning\ hai-data.xlsx', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\ MachineLearning\ hai-data.csv', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\ MachineLearning\ hai-data.csv', stringsAsFactors = F)
train <- read.csv('C:\ codingProjects\ hai-data.csv', stringsAsFactors = F)
q()
q()
library('ggplot2')
library('ggthemes')
library('scales'
library('scales')
library('scales')
library('dplyr')
library('randomForest')
train <- read.csv('C:\ codingProjects\ MachineLearning\ hai_data.csv', stringsAsFactors = F)
train <- read.csv('C:/codingProjects/MachineLearning/hai_data.csv', stringsAsFactors = F)
full <- train
set.seed(123)
train_index <- sample(seq_len(nrow(full)), size=0.8 * nrow(full))
train <- full[train_index,] #80% of training data
test <- full[-train_index,] #20% of trainng data
str(train)
str(test)
# Load packages, Do Not Change.
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('randomForest') # classification algorithm
# NOTE: Take a look at the sample coding that we had in class.
# Nothing here is new. I do not ask for your programming skill here.  
# Look at your dataset: It's a modified HAI dataset from the class. The new column "mode" is 0 or 1.
# 0 means normal, 1 means abnormal (e.g., malfunction).
train <- read.csv('C:/codingProjects/MachineLearning/hai_data.csv', stringsAsFactors = F)
full <- train
set.seed(123)
train_index <- sample(seq_len(nrow(full)), size=0.8 * nrow(full))
train <- full[train_index,] #80% of training data
test <- full[-train_index,] #20% of trainng data
# Display your datasets. Do Not Change.
str(train)
str(test)
### STEP 2: Ready for prediction ###
# STEP 2-1: Set a random seed
# show your code below
set.seed(10)
# STEP 2-2: Build your random forest model
# Identify your target feature to predict and the rest.
# STEP 2-2: Build your random forest model
# Identify your target feature to predict and the rest.
train_data <- train[, c("mode", "P1_FCV01D", "P1_FCV02D", "P1_FCV03D", "P1_FT01", "P1_FT02", "P1_FT03")]
rf_model <- randomForest(mode ~ ., data = train_data, ntree = 100)
train_data$P1_FCV01D <- (train_data$P1_FCV01D - min(train_data$P1_FCV01D)) / (max(train_data$P1_FCV01D) - min(train_data$P1_FCV01D))
predictor_columns <- names(train_data)[names(train_data) != "mode"]
for (col in predictor_columns) {
  train_data[[col]] <- (train_data[[col]] - min(train_data[[col]])) / 
                       (max(train_data[[col]]) - min(train_data[[col]]))
}
rf_model <- randomForest(mode ~ ., data = train_data, ntree = 100)
train_data$mode <- as.factor(train_data$mode)
rf_model <- randomForest(mode ~ ., data = train_data, ntree = 100)
print(rf_model)
### STEP 3: Model error and analysis. ###
# Show model error, calculate importance, and rank valuable (it is saved to Rplot.pdf)
# show your code below
print(rf_model)
importance_values <- importance(rf_model)
print(importance_values)
importance_df <- as.data.frame(importance_values)
importance_df$Feature <- rownames(importance_df)
ranked_importance <- importance_df[order(importance_df$MeanDecreaseGini, decreasing = TRUE), ]
print(ranked_importance)
pdf("Rplot.pdf")
varImpPlot(rf_model, main="Feature Importance Ranking")
dev.off()
pdf("Rplot.pdf")
varImpPlot(rf_model, main="Feature Importance Ranking")
dev.off()
getwd()
prediction <- predict(rf_model, test)
results <- data.frame(
  your_prediction = prediction,
  ground_truth = test$mode  # Assuming 'mode' is the target column in test data
)
write.csv(results, "hai-test-accuracy.csv", row.names = FALSE)
q()
# title: "Basic_COVID_Analysis"
# author: "Sumin Cho"
# output: html_document
library(Hmisc)
install.packages(Hmisc)
install.packages("Hmisc")
library("Hmisc")
covid_data <- read.csv("C:\Users\sumin\Downloads\archive\covid_19_data.csv)
covid_data <- read.csv("C:\Users\sumin\Downloads\archive\covid_19_data.csv")
covid_data <- read.csv("C:/Users/sumin/Downloads/archive/covid_19_data.csv")
head(covid_data)
q()
install.packages(c("tydiverse", "lubridate", "ggplot2", "dplyr", "readr", "forecast", "scales"))
library(readr)
covid_data <- read_csv("data/owid-covid-data.csv")
covid_data
str(covid_data_data)
str(covid_data)
summary(covid_data)
library(dplyr)
covid_filtered <- covid_data %>%
select(date, location, total_case, total-deaths, people_vaccinated, population) %>%
q()
q()
